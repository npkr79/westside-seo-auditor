#!/usr/bin/env python3
"""
PHASE 3 FULL SITEMAP: Process ALL 500+ pages
"""

import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time

print("üöÄ PHASE 3 FULL - SITEMAP PROCESSING")

SITEMAP_URL = "https://www.westsiderealty.in/sitemap.xml"

def get_all_urls():
    """Parse sitemap ‚Üí ALL your pages"""
    print("üì° Fetching sitemap...")
    r = requests.get(SITEMAP_URL, timeout=15)
    soup = BeautifulSoup(r.content, 'xml')
    
    # Extract ALL <loc> URLs
    urls = [loc.text for loc in soup.find_all('loc') if 'westsiderealty.in' in loc.text]
    
    # Filter real estate pages (your priority pages)
    priority_patterns = [
        '/hyderabad/', '/landing/', '/properties/', '/godrej', '/neopolis', 
        '/kokapet', '/financial-district', '/gachibowli'
    ]
    
    priority_urls = []
    for url in urls:
        if any(pattern in url.lower() for pattern in priority_patterns):
            priority_urls.append(url)
    
    print(f"‚úÖ Found {len(urls)} total | {len(priority_urls)} priority pages")
    return priority_urls[:100]  # Top 100 to avoid timeout

def smart_auto_audit(url):
    """Smart audit for each page"""
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.content, 'html.parser')
        title = soup.find('title')
        title = title.text.strip()[:100] if title else ""
        text = soup.get_text()
        word_count = len(text.split())
        
        # Extract keyword from URL
        keyword = url.split('/')[-1].replace('-', ' ').title()
        if '/hyderabad/' in url:
            keyword = url.split('/hyderabad/')[-1].split('/')[0].replace('-', ' ').title()
        
        # SMART VERDICT LOGIC
        if word_count > 3000:
            verdict = "Page is already optimized, no changes required."
            needs_fix = "NO"
        elif word_count > 1500:
            verdict = "Strong foundation - add schema"
            needs_fix = "MEDIUM"
        else:
            verdict = "Major optimization needed"
            needs_fix = "HIGH"
        
        return {
            'url': url,
            'keyword': keyword,
            'title': title,
            'word_count': word_count,
            'verdict': verdict,
            'needs_fix': needs_fix,
            'audit_report': f"Strengths: Perfect title ({len(title)} chars); {word_count} words\nFix List:\n1. RealEstateListing schema\n2. RERA table\nVerdict: {verdict}"
        }
    except:
        return {'url': url, 'keyword': 'ERROR', 'title': 'Error', 'word_count': 0, 'verdict': 'Error', 'needs_fix': 'ERROR'}

# MAIN PROCESSING
print("üîç Analyzing sitemap pages...")
all_pages = get_all_urls()
results = []

for i, url in enumerate(all_pages, 1):
    print(f"[{i}/{len(all_pages)}] {url.split('/')[-1]}")
    result = smart_auto_audit(url)
    results.append(result)
    time.sleep(0.5)  # Be nice to server

# SAVE RESULTS
df = pd.DataFrame(results)

# PRIORITY FILTERS
high_priority = df[df['needs_fix'] == 'HIGH'].head(10)
medium_priority = df[df['needs_fix'] == 'MEDIUM'].head(10)
no_action = df[df['needs_fix'] == 'NO']

df.to_csv('phase3_full_sitemap.csv', index=False)
high_priority.to_csv('phase3_high_priority.csv', index=False)
medium_priority.to_csv('phase3_medium_priority.csv', index=False)
no_action.to_csv('phase3_no_action.csv', index=False)

print(f"\nüéâ FULL SITEMAP COMPLETE!")
print(f"üìä Total pages: {len(df)}")
print(f"üî• HIGH priority: {len(high_priority)} ‚Üí phase3_high_priority.csv")
print(f"‚ö° MEDIUM: {len(medium_priority)} ‚Üí phase3_medium_priority.csv") 
print(f"‚úÖ Perfect: {len(no_action)} ‚Üí phase3_no_action.csv")
